# Human Evaluation of TextToKG BEL Extraction

This folder contains the human-evaluation framework and results for assessing the quality of BEL statements generated by our **TextToKG** LLM tool.  
Five domain experts independently reviewed the same set of BEL statements against predefined criteria to establish inter-rater agreement and identify common error patterns.

---

## Overview of the Evaluation Workflow

1. **Multiple reviewers** independently score the same BEL statements.  
2. **Consensus analysis** aggregates scores, detects agreement patterns and common errors.  
3. **Visualization & deep-dive notebook** explores reviewer agreement, error distributions, and actionable insights.

---

## Folder Structure

```text
reviewers_results_comparison/
├── reviewers_results/
│   ├── augustin_review.json
│   ├── augustin_rev.png
│   ├── clara_review.json
│   ├── clara_rev.png
│   ├── jungho_review.json
│   ├── jungho_rev.png
│   ├── nicole_review.json
│   ├── nicole_rev.png
│   └── zhao_review.json
├── comp_reviews.py
├── plot_scores.py
├── consensus_analysis.ipynb
└── review_consensus_with_evidence.csv
```

---

## File Descriptions

### Review Data (`reviewers_results/`)

| File                     | Description                                              |
| ------------------------ | -------------------------------------------------------- |
| `[reviewer]_review.json` | Raw scores & comments for each BEL statement.            |
| `[reviewer]_rev.png`     | Quick-look plot of that reviewer's scoring distribution. |

### Analysis Scripts

| Script                | Purpose                                                                                              |
| --------------------- | ---------------------------------------------------------------------------------------------------- |
| **`comp_reviews.py`** | Aggregates all reviews, computes consensus metrics, and writes `review_consensus_with_evidence.csv`. |
| **`plot_scores.py`**  | Generates static PNG visualizations of global agreement/error distributions for quick reporting.     |

### **`consensus_analysis.ipynb`**  

A fully-explained Jupyter notebook for interactive exploration of reviewer data.

| Notebook Section               | What it does                                                                                        |
| ------------------------------ | --------------------------------------------------------------------------------------------------- |
| **1. Load Data**               | Reads `review_consensus_with_evidence.csv` plus individual JSONs.                                   |
| **2. Summary Metrics**         | Calculates per-statement *All-Correct counts*, majority consensus, and reviewer-level stats.        |
| **3. Agreement Heatmap**       | Reviewer × Reviewer matrix of pairwise agreement rates.                                             |
| **4. Error Distribution**      | Bar and pie charts of the most frequent error types.                                                |
| **5. Fingerprint Clustering**  | Fingerprints each review (binary error codes) and clusters statements to surface systematic issues. |
| **6. Recommendations**         | Automatically highlights BEL functions or namespaces that trigger the most disagreement.            |

> The notebook uses **pandas**, **numpy**, **matplotlib**, and **seaborn** only—no heavy dependencies.

---

## Analysis Outputs

| File                                          | Contents                                                                                                                                                |
| --------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **`review_consensus_with_evidence.csv`**      | One row per BEL statement with: Index, BEL statement, evidence text, All-Correct count, Consensus count, agreed criteria, and aggregated error summary. |
| **`[reviewer]_rev.png`**                      | Individual score histograms.                                                                                                                            |
| **Figures inside `consensus_analysis.ipynb`** | • Agreement heatmap• Error frequency bar chart• Reviewer-statement cluster map                                    |

---

## Evaluation Criteria

Reviewers assessed each BEL statement against the following error criteria:

### Error Types Evaluated

1. **`incorrect_entities`** - The entities (subject/object) are incorrectly identified or extracted
2. **`not_a_valid_relationshiptype`** - The relationship type is not a valid BEL predicate
3. **`interaction_type_errors`** - The type of biological interaction is misrepresented
4. **`identifier_database_errors`** - Incorrect biological database identifiers or namespace assignments
5. **`correct_but_could_be_more_precise`** - The statement is technically correct but lacks specificity
6. **`other_errors`** - Miscellaneous errors not covered by other categories
7. **`all_correct`** - Special flag indicating the statement has no errors

Each reviewer could mark multiple error types per statement, creating an error "fingerprint" used for consensus analysis.

---

## Key Metrics

### Inter-Rater Agreement

| Term                   | Meaning                                        |
| ---------------------- | ---------------------------------------------- |
| **Perfect Agreement**  | All 5 reviewers give identical scores.         |
| **Majority Consensus** | ≥3 reviewers share the same error fingerprint. |
| **High Disagreement**  | No shared fingerprint among ≥3 reviewers.      |

### Error Patterns

| Pattern               | Significance                                                    |
| --------------------- | --------------------------------------------------------------- |
| **Common Errors**     | Frequently tagged issues—priority fixes.                        |
| **Reviewer-Specific** | Outlier tags—double-check guidelines or reviewer understanding. |

---

## How to Reproduce

### 1 · Run Consensus Aggregation

```bash
python comp_reviews.py
```

Creates `review_consensus_with_evidence.csv`.

### 2 · Generate Quick Plots

```bash
python plot_scores.py
```

Produces `[reviewer]_rev.png` plus aggregate figures in the root folder.

### 3 · Interactive Exploration

```bash
jupyter notebook consensus_analysis.ipynb
```

Run all cells or step through to customize visuals.

---

## Interpreting Results

* **All-Correct Count 5/5** → unanimous correctness
* **Consensus Count 4-5** → strong evidence of systematic error (or correctness)
* **Low Consensus** → ambiguous text or reviewer criterion mismatch; consider clarifying guidelines

The **Error Summary** column in the CSV and the bar charts in the notebook highlight which BEL functions, namespaces, or relationship types cause the most problems—prime targets for model-prompt refinement.

---

## Future Improvements

1. Weight scoring by error severity.
2. Track temporal changes (reviewer fatigue trends).
3. Auto-flag high-disagreement statements for team review.
4. Merge results with INDRA-vs-LLM comparison for a holistic evaluation dashboard.

---

*Questions or suggestions?*
Open an issue or start a discussion in this repository!
